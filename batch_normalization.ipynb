{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "    Batch normalization is most useful when building deep neural networks. To demonstrate this, we will create a convolutional neural network with 20 convolutional layers, followed by a fully connected layer. We will use it to classify handwritten digits in the MNIST dataset, which should be familiar to you by now.\n",
    "    \n",
    "    This notebook includes 2 versions of the network. First, uses higher level functions from tf.layers. Second, uses tf.nn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import packages\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#read data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True, reshape = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization using tf.layers.batch_normalization\n",
    "    this version of the network uses tf.layers for everything, and excepts you to implement batch normalization using tf.layers.batch_normalization\n",
    "    \n",
    "    We will use the following function to create fully connected layers in our network. We will create them with the specified number of neurons and a ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units):\n",
    "    '''\n",
    "    create a fully connected layer with the given layer as input and the given number of neurons\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    prev_layer, tensor: The tensor acts as input into this layer\n",
    "    num_units, int: number of the units, nodes, neurons\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    layer, tensor; a fully connected layer\n",
    "    '''\n",
    "    \n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation = tf.nn.relu)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We will use the following function to create convolutional layers in our network. We are using a 3x3 kernel, ReLu activation functions, strides of 1x1 on layers with odd depths, and strides of 2x2 on layers with even depths. We dont use pooling layers at all in this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth):\n",
    "    '''\n",
    "    create a convolutional layer with the given layer as input\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    prev_layer, tensor: input into this layer\n",
    "    layer_depth, int: we will set the strides and number of feature maps based on the layer depth of the network\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    conv_layer, tensor: a new convolutional layer\n",
    "    '''\n",
    "    \n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth * 4, 3, strides, 'same', activation = tf.nn.relu)\n",
    "    \n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This cell builds the network without batch normalization, then it trains it on the MNIST dataset, It displays loss and accuracy data prediocally while training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CNN with fully connected layer as output that created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0: validation loss: 0.691, validation accuracy: 0.099\n",
      "batch: 25: training training: 0.481, training accuracy: 0.078\n",
      "batch: 50: training training: 0.331, training accuracy: 0.109\n",
      "batch: 75: training training: 0.327, training accuracy: 0.062\n",
      "batch: 100: validation loss: 0.325, validation accuracy: 0.175\n",
      "batch: 125: training training: 0.326, training accuracy: 0.031\n",
      "batch: 150: training training: 0.326, training accuracy: 0.141\n",
      "batch: 175: training training: 0.323, training accuracy: 0.125\n",
      "batch: 200: validation loss: 0.326, validation accuracy: 0.099\n",
      "batch: 225: training training: 0.325, training accuracy: 0.203\n",
      "batch: 250: training training: 0.326, training accuracy: 0.078\n",
      "batch: 275: training training: 0.323, training accuracy: 0.125\n",
      "batch: 300: validation loss: 0.326, validation accuracy: 0.092\n",
      "batch: 325: training training: 0.323, training accuracy: 0.156\n",
      "batch: 350: training training: 0.323, training accuracy: 0.125\n",
      "batch: 375: training training: 0.326, training accuracy: 0.094\n",
      "batch: 400: validation loss: 0.325, validation accuracy: 0.113\n",
      "batch: 425: training training: 0.325, training accuracy: 0.094\n",
      "batch: 450: training training: 0.326, training accuracy: 0.109\n",
      "batch: 475: training training: 0.325, training accuracy: 0.047\n",
      "batch: 500: validation loss: 0.325, validation accuracy: 0.100\n",
      "batch: 525: training training: 0.327, training accuracy: 0.125\n",
      "batch: 550: training training: 0.321, training accuracy: 0.203\n",
      "batch: 575: training training: 0.321, training accuracy: 0.125\n",
      "batch: 600: validation loss: 0.314, validation accuracy: 0.217\n",
      "batch: 625: training training: 0.325, training accuracy: 0.141\n",
      "batch: 650: training training: 0.323, training accuracy: 0.156\n",
      "batch: 675: training training: 0.295, training accuracy: 0.219\n",
      "batch: 700: validation loss: 0.297, validation accuracy: 0.203\n",
      "batch: 725: training training: 0.276, training accuracy: 0.297\n",
      "batch: 750: training training: 0.257, training accuracy: 0.344\n",
      "batch: 775: training training: 0.229, training accuracy: 0.438\n",
      "final validation accuracy: 0.39\n",
      "final test accuracy: 0.38\n",
      "accuracy on 100 samples: 0.39\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "a CNN without Batch Normalization\n",
    "'''\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    #build placeholders for the input samples and labels\n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    #feed the inputs into a series of 20 convolutional layers\n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "        \n",
    "    #flatten the output from the convolutional layers\n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape = [-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "    \n",
    "    #add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "    \n",
    "    #create the output layer from the convolutional layers\n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    #define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    #create operations to test operations\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    #train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            #train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            #periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images, \n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print(\"batch: {}: validation loss: {:.3f}, validation accuracy: {:.3f}\".\n",
    "                     format(batch_i, loss, acc))\n",
    "                \n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print(\"batch: {}: training training: {:.3f}, training accuracy: {:.3f}\".\n",
    "                      format(batch_i, loss, acc))\n",
    "                \n",
    "        #at the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images, labels: mnist.validation.labels})\n",
    "        print(\"final validation accuracy: {:.2f}\".format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images, labels: mnist.test.labels})\n",
    "        print(\"final test accuracy: {:.2f}\".format(acc))\n",
    "        \n",
    "        #score the first 100 test images individually\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy, feed_dict = {inputs: [mnist.test.images[i]], \n",
    "                                                       labels: [mnist.test.labels[i]]})\n",
    "            \n",
    "        print(\"accuracy on 100 samples: {:.2f}\".format(correct / 100))\n",
    "        \n",
    "\n",
    "'''\n",
    "hyperparameters\n",
    "'''\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add batch normalization\n",
    "    modify fully_connected and conv_layer function and add batch normalization to the fully connected layers it creates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "same fully_connected function but added batch normalization\n",
    "'''\n",
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    '''\n",
    "    create a fully connected layer with the given layer as input and the given number of neurons\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    prev_layer, tensor: acts as input into this layer\n",
    "    num_units, int: number of units, nodes, neurons\n",
    "    is_training, bool: batch_normalization parameter\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    layer, tensor: fully connected layer with batch normalization applied\n",
    "    '''\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias = False, activation = None)\n",
    "    layer = tf.layers.batch_normalization(layer, training = is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "same conv_layer function but added batch normalization\n",
    "'''\n",
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    '''\n",
    "    create a convolutional layer with the given layer as input\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    prev_layer, tensor: that acts as input into this layer\n",
    "    layer_depth, int: we wil set the strides and number of features maps based on the layers depth\n",
    "    is_training, bool: batch_normalization parameters\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "    conv_layer, tensor: a convolutional layer with batch normalization added\n",
    "    '''\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth * 4, 3, strides, 'same', use_bias = False, activation = None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training = is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a CNN network with a fully conneted layer as output now with added batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0 validation loss: 0.69 validation accuracy: 0.10\n",
      "batch: 25 training loss: 0.58 training accuracy: 0.08\n",
      "batch: 50 training loss: 0.45 training accuracy: 0.08\n",
      "batch: 75 training loss: 0.38 training accuracy: 0.12\n",
      "batch: 100 validation loss: 0.36 validation accuracy: 0.09\n",
      "batch: 125 training loss: 0.34 training accuracy: 0.11\n",
      "batch: 150 training loss: 0.36 training accuracy: 0.08\n",
      "batch: 175 training loss: 0.32 training accuracy: 0.22\n",
      "batch: 200 validation loss: 0.29 validation accuracy: 0.34\n",
      "batch: 225 training loss: 0.19 training accuracy: 0.64\n",
      "batch: 250 training loss: 0.13 training accuracy: 0.77\n",
      "batch: 275 training loss: 0.23 training accuracy: 0.56\n",
      "batch: 300 validation loss: 0.26 validation accuracy: 0.56\n",
      "batch: 325 training loss: 0.11 training accuracy: 0.84\n",
      "batch: 350 training loss: 0.11 training accuracy: 0.88\n",
      "batch: 375 training loss: 0.06 training accuracy: 0.89\n",
      "batch: 400 validation loss: 0.07 validation accuracy: 0.90\n",
      "batch: 425 training loss: 0.01 training accuracy: 0.98\n",
      "batch: 450 training loss: 0.02 training accuracy: 0.97\n",
      "batch: 475 training loss: 0.04 training accuracy: 0.94\n",
      "batch: 500 validation loss: 0.04 validation accuracy: 0.95\n",
      "batch: 525 training loss: 0.03 training accuracy: 0.94\n",
      "batch: 550 training loss: 0.02 training accuracy: 0.98\n",
      "batch: 575 training loss: 0.11 training accuracy: 0.84\n",
      "batch: 600 validation loss: 0.03 validation accuracy: 0.95\n",
      "batch: 625 training loss: 0.02 training accuracy: 0.97\n",
      "batch: 650 training loss: 0.00 training accuracy: 1.00\n",
      "batch: 675 training loss: 0.07 training accuracy: 0.89\n",
      "batch: 700 validation loss: 0.03 validation accuracy: 0.96\n",
      "batch: 725 training loss: 0.04 training accuracy: 0.95\n",
      "batch: 750 training loss: 0.04 training accuracy: 0.91\n",
      "batch: 775 training loss: 0.02 training accuracy: 0.97\n",
      "final validation accuracy: 0.96\n",
      "final test accuracy: 0.96\n",
      "accuracy on 100 samples: 0.99\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "modify the network that will be trained with batch normalization\n",
    "'''\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    #build placeholder for the input samples and labels\n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    #add placeholder to indicate whether or not we are training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    #feed the inputs into a series of 20 convolutional layers\n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "        \n",
    "    #flatten the output from convolutional layers\n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape = [-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "    \n",
    "    #add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "    \n",
    "    #create the output layer with 1 node for each\n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    #define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    '''\n",
    "    tell tensorflow to update the population statistics while training\n",
    "    '''\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "        \n",
    "    #create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    #train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            #train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            #periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images, \n",
    "                                                              labels: mnist.validation.labels, \n",
    "                                                              is_training: False})\n",
    "                print(\"batch: {} validation loss: {:.2f} validation accuracy: {:.2f}\".format(batch_i, loss, acc))\n",
    "                \n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys,\n",
    "                                                              is_training: False})\n",
    "                print(\"batch: {} training loss: {:.2f} training accuracy: {:.2f}\".format(batch_i, loss, acc))\n",
    "                \n",
    "        #score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images, \n",
    "                                  labels: mnist.validation.labels, \n",
    "                                  is_training: False})\n",
    "        print(\"final validation accuracy: {:.2f}\".format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images, \n",
    "                                  labels: mnist.test.labels, \n",
    "                                  is_training: False})\n",
    "        print(\"final test accuracy: {:.2f}\".format(acc))\n",
    "        \n",
    "        #score the first 100 test images individually, just to make sure batch normalization worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy, feed_dict = {inputs: [mnist.test.images[i]], \n",
    "                                                    labels: [mnist.test.labels[i]], \n",
    "                                                    is_training: False})\n",
    "            \n",
    "        print(\"accuracy on 100 samples: {:.2f}\".format(correct / 100))\n",
    "        \n",
    "'''\n",
    "hyperparameters\n",
    "'''\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
